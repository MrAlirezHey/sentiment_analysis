# -*- coding: utf-8 -*-
"""sentiment_analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ogHnbBwFnTXMOiPcMfCFhIuXMmxi2l00
"""

!pip install datasets evaluate

!pip install --upgrade datasets huggingface-hub fsspec

from transformers import  BertTokenizer
from datasets import load_dataset
dataset=load_dataset("imdb")
tokenizer=BertTokenizer.from_pretrained('bert-base-uncased')
def tokenizer_function(samples):
  return tokenizer(samples['text'],padding='max_length',truncation=True,max_length=512)
tokenize_dataset=dataset.map(tokenizer_function,batched=True)
tokenize_dataset=tokenize_dataset.rename_column('label','labels')
tokenize_dataset.set_format(type='torch',columns=['input_ids','attention_mask','labels'])
print(tokenize_dataset)

train_dataset=tokenize_dataset['train']
test_dataset=tokenize_dataset['test']

import torch
from transformers import BertModel
import torch.nn as nn
bert=BertModel.from_pretrained('bert-base-uncased')
class BertClassifier(nn.Module):
  def __init__(self,bert_model):
    super(BertClassifier,self).__init__()
    self.bert=bert_model
    for param in self.bert.parameters():
      param.requires_grad=False
    self.fc1=nn.Linear(768,1024)
    self.relu=nn.ReLU()
    self.fc2=nn.Linear(1024,2)
  def forward(self,input_ids,attention_mask):
    outputs=self.bert(input_ids=input_ids,attention_mask=attention_mask)
    pooled_output=outputs.pooler_output
    hidden_output=self.relu(self.fc1(pooled_output))
    logits=self.fc2(hidden_output)
    return logits
model=BertClassifier(bert)
device='cuda' if torch.cuda.is_available() else 'cpu'
model.to(device)

from torch.utils.data import DataLoader
import torch.optim as optim
train_dataloder=DataLoader(train_dataset,shuffle=True,batch_size=16)
test_dataloader=DataLoader(test_dataset,batch_size=32)
criterion=nn.CrossEntropyLoss()
optimizer=optim.Adam(list(model.fc1.parameters())+list(model.fc2.parameters()),lr=5e-5)

from tqdm.auto import tqdm
num_epochs=10
progsess_bar=tqdm(range(num_epochs))
for epoch in range(num_epochs):
  model.train()
  total_loss,correct,total=0,0,0
  for batch in train_dataloder:
    input_ids=batch['input_ids'].to(device)
    attention_mask=batch['attention_mask'].to(device)
    labels=batch['labels'].to(device)
    optimizer.zero_grad()
    logits=model(input_ids,attention_mask)
    loss=criterion(logits,labels)
    loss.backward()
    optimizer.step()
    total_loss+=loss.item()
    correct+=(logits.argmax(dim=1)==labels).sum().item()
    total+=labels.size(0)
  progsess_bar.update(1)
  print(f'Epoch {epoch+1}: Loss = {total_loss/len(train_dataloder):.4f},Accuracy = {correct/total:.4f}')

model.eval()
correct , total = 0 , 0
with torch.no_grad():
  for batch in test_dataloader:
    input_ids=batch['input_ids'].to(device)
    attention_mask=batch['attention_mask'].to(device)
    labels=batch['labels'].to(device)
    logits=model(input_ids,attention_mask)
    predictions=logits.argmax(dim=1)
    correct+=(predictions==labels).sum().item()
    total+=labels.size(0)
print(f'Validation Accurave : {correct/total:.4f}')

def predict_sentimen(sentence):
  inputs=tokenizer(sentence,return_tensors='pt',padding=True,truncation=True,max_length=512)
  input_ids=inputs['input_ids'].to(device)
  attention_mask=inputs['attention_mask'].to(device)
  model.eval()
  with torch.no_grad():
    logits=model(input_ids,attention_mask)
  prediction=logits.argmax(dim=1).item()
  if prediction ==1:
    print('sentence : Posetive')
  else:
    print('sentence : Negative')
  return prediction